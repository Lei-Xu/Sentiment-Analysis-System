import os
import sys
import ast
import math

from preprocessor import tokenize
from preprocessor import get_files

# N represents the total number of documents
N = len([f for f in os.listdir('extracted_files') if os.path.isfile(os.path.join('extracted_files', f))])


def add_to_dictionary(dictionary, term):
    """
    :param dictionary: dictionary of tokens, 'term1': ['newid1', 'newid2], 'term2': ['newid1', 'newid2]
    :param term: term from a token tuple that is not existing in the current dictionary
    :return: newly created postings_list from the disk
    """
    dictionary[term] = []
    return dictionary[term]


def get_postings_list(dictionary, term):
    """
    :param dictionary: dictionary of tokens, 'term1': ['newid1', 'newid2], 'term2': ['newid1', 'newid2]
    :param term: term from a token tuple
    :return: specific postings_list from the disk
    """
    return dictionary[term]


def add_to_postings_list(postings_list, newid, term):
    """
    :param postings_list: specific postings_list from the disk
    :param newid: id of the document
    :param term: term from a token tuple
    """
    sentiment_value = 0
    term_frequency = 1
    afinn = dict(map(lambda p: (p[0], int(p[1])), [line.split('\t') for line in open("AFINN-111.txt")]))
    for key in afinn.keys():
        if key == term:
            sentiment_value = afinn[key]
            break
    for item in postings_list:
        if item[0] == newid:
            item[1] += 1
            item[3] = (1 + math.log(item[1]))*(math.log(N/len(postings_list)))
            return
    document_frequency = len(postings_list) + 1
    tf_idf_weight = (1 + math.log(term_frequency))*(math.log(N/document_frequency))
    postings_list.append([newid, term_frequency, sentiment_value, tf_idf_weight])


def sort_terms(dictionary):
    """
    :param dictionary: dictionary of tokens, 'term1': ['newid1', 'newid2], 'term2': ['newid1', 'newid2]
    :return: list of sorted terms in the disk
    """
    sorted_terms = []
    for key in sorted(dictionary.keys()):
        sorted_terms.append(key)
    return sorted_terms


def write_block_to_disk(sorted_terms, disk, file_name, path):
    """
    :param path: path to write data
    :param sorted_terms: list of sorted terms in the disk
    :param disk: dictionary of tokens, 'term1': ['newid1', 'newid2], 'term2': ['newid1', 'newid2]
    :param file_name: name of outputting block file
    """
    print('Writing ' + file_name + ' into disk...')
    current_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(current_dir, path)
    try:
        os.makedirs(output_dir)
    except OSError:
        pass
    with open(path + '/' + file_name + '.txt', 'w') as block:
        for term in sorted_terms:
            block.write(term + ': ' + str(disk[term]) + '\n')


def spimi_invert(token_id_pairs, path='blocks', block_size=float(1)):
    """
    Designed based on SPIMI algorithm from textbook
    :param token_id_pairs: list of tokens in form of tuples ('token', 'newid') generated by preprocessor
    :param block_size: disk memory limitation in terms of MB, default value is 0.5 MB
    """
    block_count = 0
    disk = {}
    for index, token in enumerate(token_id_pairs):
        token_id_tuple = token_id_pairs[index]

        if token_id_tuple[0] not in disk:
            postings_list = add_to_dictionary(disk, token_id_tuple[0])
        else:
            postings_list = get_postings_list(disk, token_id_tuple[0])

        add_to_postings_list(postings_list, token_id_tuple[1], token_id_tuple[0])
        # check if the size of the current disk is reached to the limit or if reaching last token
        if sys.getsizeof(disk)/1024/1024 >= block_size or index + 1 == len(token_id_pairs):
            print('Sorting terms in ' + str('BLOCK' + str(block_count + 1)))
            sorted_terms = sort_terms(disk)
            write_block_to_disk(sorted_terms, disk, str('BLOCK' + str(block_count + 1)), path)
            disk.clear()
            block_count += 1


def merge_blocks(path='blocks', output_name='FINAL_INDEX.txt'):
    # open all block files concurrently
    block_files = {index: open(file_path, 'r') for index, file_path in enumerate(os.scandir(path + '/')) if not file_path.name.split('/')[0].startswith('.')}
    # new file to hold all data with write and read mode
    final_index_file = open(output_name, 'w+')
    # dictionary in which will hold all current lines from all the block files with their index as key
    lines_holder = {}
    # list holding current lines terms
    terms_holder = []
    # list of file in which term occurred
    occurrence_holder = []
    # flag indicate no more line in all files
    no_more_lines = False

    # reading first line in each block_file and store into lines_holder
    for block_index in block_files:
        print('Started reading from ' + block_files[block_index].name + '...')
        current_line = block_files[block_index].readline()
        # split term and newid using rsplit() to split on last occurrence in case token contains ':'
        current_line_splited = current_line.rsplit(':', 1)
        # transform string of posting list into real list
        posting_list = ast.literal_eval(current_line_splited[1].strip())
        # transform current_line into a dictionary -> {term: posting_list}
        current_line_dictionary = {current_line_splited[0]: posting_list}
        # push line into holder -> {index: current_line_dictionary}
        lines_holder[block_index] = current_line_dictionary
        # push term into holder
        terms_holder.append(current_line_splited[0])
    while not no_more_lines:
        # get the current term to write into final index
        current_term_to_write = sorted(terms_holder)[0]
        posting_list_to_write = []
        # get list of occurrence
        for file_index in lines_holder:
            if current_term_to_write in lines_holder[file_index]:
                occurrence_holder.append(file_index)
        # merge posting list of terms
        for occurrence in occurrence_holder:
            posting_list_to_write += lines_holder[occurrence][current_term_to_write]
        # write line into finale index file
        final_index_file.write(str(current_term_to_write) + ': ' + str(posting_list_to_write) + '\n')
        # remove term that already been written in file
        terms_holder = list(filter(lambda term: term != current_term_to_write, terms_holder))
        # read new line for block file that had terms occurred
        for occurrence in occurrence_holder:
            current_line = block_files[occurrence].readline()
            if not current_line == '':
                # repeat previous process
                current_line_splited = current_line.rsplit(':', 1)
                posting_list = ast.literal_eval(current_line_splited[1].strip())
                current_line_dictionary = {current_line_splited[0]: posting_list}
                lines_holder[occurrence] = current_line_dictionary
                terms_holder.append(current_line_splited[0])
            else:
                # remove block file in the list
                print('All content from ' + block_files[occurrence].name + ' is merged into ' + output_name + '...')
                del lines_holder[occurrence]
                del block_files[occurrence]
        occurrence_holder.clear()
        if len(block_files) == 0 or len(lines_holder) == 0:
            print('All block files are merged into ' + output_name + '...')
            no_more_lines = True


if __name__ == '__main__':
    block_size = input('Please select your desired disk limit in MB (default is 1 MB): ')
    spimi_invert(tokenize(get_files('extracted_files')), 'blocks', float(block_size))
    merge_blocks('blocks', 'FINAL_INDEX.txt')
